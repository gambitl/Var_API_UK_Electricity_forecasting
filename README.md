# ğŸ“ˆ Projet de prÃ©diction du prix du cours du gaz naturel au Royaume-Uni

Ce projet se focalise sur la mise en production d'un modÃ¨le de sÃ©ries temporelles multivariÃ© visant Ã  prÃ©dire le prix du cours du gaz naturel au Royaume-Uni. Le modÃ¨le est rÃ©alisÃ© en utilisant la librairie `statsmodels`.

**Attention** : Ã  date, l'application souffre d'un problÃ¨me d'affichage d'images et de styling css (28/05/2023).
Vous pouvez tester le styling en exÃ©cutant le code en local :
    1. Rendez-vous dans le fichier main.py dans le dossier web_app
    2. Lancez le proxy online via SSH (voir config & dÃ©ploiement, point 5)
    3. Modifiez la toute derniÃ¨re ligne
## Architecture

Le projet est mis en production sur Google Cloud Platform (GCP) en utilisant les services suivants :

1. ğŸ“‚ **Google Cloud Storage** : les donnÃ©es d'entraÃ®nement sont stockÃ©es dans un bucket Google Cloud Storage dÃ©diÃ©.
2. ğŸ³ **API et container Docker** : le modÃ¨le est rendu accessible au public via une API hÃ©bergÃ©e dans un container Docker. Il expose une page HTML.
3. ğŸ”¨ **Google Cloud Build** : intÃ©gration continue et dÃ©ploiement du container Docker grÃ¢ce Ã  Google Cloud Build.
4. â˜ï¸ **Google Cloud Run** : dÃ©ploiement du container Docker sur Google Cloud Run, permettant d'exÃ©cuter l'API.
5. ğŸ›ï¸ **Google Cloud Composer** : le fichier Python du DAG Airflow, situÃ© dans le dossier "dag_airflow" du projet, est utilisÃ© pour rÃ©-entraÃ®ner le modÃ¨le toutes les heures. Le DAG s'exÃ©cute sur la brique Google Cloud Composer.
6. ğŸ“Š **MLFlow** : toutes les mÃ©triques d'entraÃ®nement et les artefacts, tels que le modÃ¨le, sont sauvegardÃ©s et historisÃ©s en utilisant la librairie Python MLFlow. Les donnÃ©es de MLFlow sont stockÃ©es dans un autre bucket Google Cloud Storage dÃ©diÃ©.
7. ğŸ—„ï¸ **PostgreSQL** : les donnÃ©es de MLFlow sont Ã©galement sauvegardÃ©es dans une base de donnÃ©es PostgreSQL.
8. ğŸ’» **Google Cloud Compute Engine** : la brique MLFlow s'exÃ©cute sur une machine virtuelle crÃ©Ã©e Ã  l'aide de Google Cloud Compute Engine.

## Tests de la pipeline

La pipeline est rigoureusement testÃ©e pour assurer la qualitÃ© des prÃ©dictions et des donnÃ©es utilisÃ©es, en se concentrant sur les aspects suivants :

1. ğŸ“ **Tests des requÃªtes API** : les terminaisons des requÃªtes API sont testÃ©es pour s'assurer de la disponibilitÃ© et de la fiabilitÃ© de l'API.
2. ğŸ“Š **Tests des donnÃ©es statistiques du jeu d'entraÃ®nement** : les donnÃ©es d'entraÃ®nement sont soumises Ã  des tests statistiques pour vÃ©rifier leur cohÃ©rence et leur adÃ©quation aux prÃ©dictions.
3. ğŸ§ª **Tests de qualitÃ© des donnÃ©es** : des tests de qualitÃ© sont effectuÃ©s sur les donnÃ©es utilisÃ©es, tels que des contrÃ´les de validitÃ©, de complÃ©tude et de cohÃ©rence.
   
**Le fichier de test se trouve dans le dossier 'test'**

## Configuration et dÃ©ploiement

1. ğŸš€ CrÃ©ez un projet sur Google Cloud Platform et configurez les services nÃ©cessaires, tels que Google Cloud Build, Google Cloud Run et Google Cloud Composer.
2. ğŸ“¦ CrÃ©ez les buckets Google Cloud Storage nÃ©cessaires, l'un pour stocker les donnÃ©es d'entraÃ®nement et l'autre pour sauvegarder les sorties de MLFlow. Mettez Ã  jour les chemins correspondants dans le code.
3. âš™ï¸ Configurez les paramÃ¨tres spÃ©cifiques du projet, tels que les identifiants d'accÃ¨s Ã  GCP, les chemins vers les donnÃ©es d'entraÃ®nement, etc.
4. ğŸ–¥ï¸ CrÃ©ez une machine virtuelle sur Google Cloud Compute Engine et configurez l'environnement MLFlow.
5. ğŸ—ƒï¸ CrÃ©ez une base de donnÃ©es PostgreSQL pour stocker les donnÃ©es de MLFlow et configurez les paramÃ¨tres de connexion dans le code. ExÃ©cutez les deux commandes suivantes pour crÃ©er un proxy entre la base et la VM, puis crÃ©er un pont :
   1. ./cloud-sql-proxy --private-ip *nom-du-projet*:*nom-de-la-bdd*
   2. mlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://*nom-de-l'instance-postgre*:*mot-de-passe-de-linstance*@*adresse-sur-laquelle-ecoute-le-proxy*:5432/*nom-de-la-bdd* --default-artifact-root gs://*nom-bucket-storageartifact*
6. ğŸ”„ Mettez en place le DAG Airflow en important dans l'interface de Composer le fichier Python du DAG situÃ© dans le dossier "dag_airflow" du projet.
7. ğŸš¢ Construisez le container Docker grÃ¢ce Ã  la brique Google Cloud Build (vous pouvez configurez une CI automatique en liant votre repo GIT comme nous l'avons fait) et dÃ©ployez-le sur Google Cloud Run.

## Lien de l'application

**Notre application est disponible ici** : https://test-api-one-shot-vilcobbika-od.a.run.app/
## Documentation

La documentation dÃ©taillÃ©e sur la construction des diffÃ©rents services cloud et les dÃ©marches Ã  suivre se trouve dans le dossier `docs` du projet. ğŸ“š
